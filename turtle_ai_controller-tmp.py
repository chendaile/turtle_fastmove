import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist
from turtlesim.msg import Pose
import math
import numpy as np
import json
import os

class ParameterizedPolicy:
    """å‚æ•°åŒ–çš„æ§åˆ¶ç­–ç•¥"""
    
    def __init__(self):
        # å¯ä¼˜åŒ–çš„ç­–ç•¥å‚æ•°ï¼ˆè¿™äº›å°±æ˜¯ç¥ç»ç½‘ç»œè¦å­¦ä¹ çš„ï¼ï¼‰
        self.params = {
            # è§’åº¦æ§åˆ¶å‚æ•°
            'angle_threshold': 0.1,          # è§’åº¦é˜ˆå€¼
            'turn_speed': 3.0,               # è½¬å‘é€Ÿåº¦
            'speed_angle_factor': 2.0,       # è½¬å‘æ—¶é€Ÿåº¦è¡°å‡å› å­
            'min_turn_speed': 0.2,           # è½¬å‘æ—¶æœ€å°é€Ÿåº¦
            
            # å¾®è°ƒå‚æ•°
            'fine_tune_gain': 2.0,           # å¾®è°ƒå¢ç›Š
            
            # è·ç¦»æ§åˆ¶å‚æ•°
            'speed_distance_factor': 1.5,    # é€Ÿåº¦-è·ç¦»ç³»æ•°
            'max_speed': 3.0,                # æœ€å¤§é€Ÿåº¦
            
            # å‡é€Ÿå‚æ•°
            'slow_distance': 0.5,            # å¼€å§‹å‡é€Ÿçš„è·ç¦»
            'slow_factor': 0.3,              # å‡é€Ÿç³»æ•°
        }
        
        # å‚æ•°è¾¹ç•Œï¼ˆé˜²æ­¢å‚æ•°è¶…å‡ºåˆç†èŒƒå›´ï¼‰
        self.param_bounds = {
            'angle_threshold': (0.02, 0.3),
            'turn_speed': (0.2, 5.0),
            'speed_angle_factor': (0.5, 10.0),
            'min_turn_speed': (0.1, 5.0),
            'fine_tune_gain': (0.5, 10.0),
            'speed_distance_factor': (0.1, 3.0),
            'max_speed': (1.0, 10.0),
            'slow_distance': (0.2, 5.0),
            'slow_factor': (0.1, 5),
        }

    def generate_action(self, distance, angle_diff):
        """ä½¿ç”¨å‚æ•°åŒ–ç­–ç•¥ç”ŸæˆåŠ¨ä½œ"""
        
        # è§’åº¦æ§åˆ¶
        if abs(angle_diff) > self.params['angle_threshold']:
            # éœ€è¦è½¬å‘
            w = self.params['turn_speed'] if angle_diff > 0 else -self.params['turn_speed']
            v = max(self.params['min_turn_speed'], 
                   self.params['max_speed'] - abs(angle_diff) * self.params['speed_angle_factor'])
        else:
            # å¯ä»¥ç›´èµ°
            w = angle_diff * self.params['fine_tune_gain']
            v = min(self.params['max_speed'], distance * self.params['speed_distance_factor'])
        
        # è·ç¦»æ§åˆ¶
        if distance < self.params['slow_distance']:
            v *= self.params['slow_factor']
        
        return v, w

    def update_params(self, param_updates):
        """æ›´æ–°å‚æ•°å¹¶ç¡®ä¿åœ¨è¾¹ç•Œå†…"""
        for param_name, update in param_updates.items():
            if param_name in self.params:
                new_value = self.params[param_name] + update
                # é™åˆ¶åœ¨è¾¹ç•Œå†…
                min_val, max_val = self.param_bounds[param_name]
                self.params[param_name] = np.clip(new_value, min_val, max_val)

    def get_param_vector(self):
        """å°†å‚æ•°è½¬æ¢ä¸ºå‘é‡ï¼ˆç”¨äºç¥ç»ç½‘ç»œï¼‰"""
        return np.array(list(self.params.values()))

class PolicyParameterNetwork:
    """ä¼˜åŒ–ç­–ç•¥å‚æ•°çš„ç¥ç»ç½‘ç»œ"""
    
    def __init__(self):
        # ç½‘ç»œç»“æ„ï¼šçŠ¶æ€(5) -> éšè—(12) -> éšè—(8) -> å‚æ•°è°ƒæ•´(9)
        self.input_size = 5   # dx, dy, distance, angle_diff, speed
        self.hidden1_size = 12
        self.hidden2_size = 8
        self.output_size = 9  # 9ä¸ªç­–ç•¥å‚æ•°çš„è°ƒæ•´å€¼
        
        # åˆå§‹åŒ–æƒé‡ï¼ˆå°ä¸€äº›ï¼Œå› ä¸ºæˆ‘ä»¬åªæ˜¯åœ¨å¾®è°ƒå‚æ•°ï¼‰
        self.w1 = np.random.randn(self.input_size, self.hidden1_size) * 0.1
        self.w2 = np.random.randn(self.hidden1_size, self.hidden2_size) * 0.1
        self.w3 = np.random.randn(self.hidden2_size, self.output_size) * 0.05
        
        self.b1 = np.random.randn(self.hidden1_size) * 0.01
        self.b2 = np.random.randn(self.hidden2_size) * 0.01
        self.b3 = np.random.randn(self.output_size) * 0.01
        
        # å­¦ä¹ å‚æ•°
        self.learning_rate = 0.0001  # æ›´å°çš„å­¦ä¹ ç‡ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨å¾®è°ƒ
        
        # å­˜å‚¨å‰å‘ä¼ æ’­ç»“æœ
        self.z1 = None
        self.a1 = None
        self.z2 = None  
        self.a2 = None
        self.z3 = None
        self.a3 = None
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        return (x > 0).astype(float)
    
    def tanh(self, x):
        return np.tanh(x)
    
    def tanh_derivative(self, x):
        return 1 - np.tanh(x)**2
    
    def forward(self, state):
        """å‰å‘ä¼ æ’­ï¼šé¢„æµ‹å‚æ•°è°ƒæ•´"""
        # å½’ä¸€åŒ–è¾“å…¥
        normalized_state = np.array([
            state[0] / 10.0,  # dx
            state[1] / 10.0,  # dy  
            state[2] / 10.0,  # distance
            state[3] / math.pi,  # angle_diff
            state[4] / 5.0    # speed
        ])
        
        # ç¬¬ä¸€éšè—å±‚
        self.z1 = np.dot(normalized_state, self.w1) + self.b1
        self.a1 = self.relu(self.z1)
        
        # ç¬¬äºŒéšè—å±‚
        self.z2 = np.dot(self.a1, self.w2) + self.b2
        self.a2 = self.relu(self.z2)
        
        # è¾“å‡ºå±‚ï¼ˆå‚æ•°è°ƒæ•´å€¼ï¼ŒèŒƒå›´[-1, 1]ï¼‰
        self.z3 = np.dot(self.a2, self.w3) + self.b3
        self.a3 = self.tanh(self.z3) * 0.1  # é™åˆ¶è°ƒæ•´å¹…åº¦
        
        return self.a3
    
    def backward(self, state, target_adjustments):
        """åå‘ä¼ æ’­"""
        # è¾“å‡ºå±‚è¯¯å·®
        output_error = self.a3 - target_adjustments
        delta3 = output_error * self.tanh_derivative(self.z3) * 0.1
        
        # ç¬¬äºŒéšè—å±‚è¯¯å·®
        error2 = np.dot(delta3, self.w3.T)
        delta2 = error2 * self.relu_derivative(self.z2)
        
        # ç¬¬ä¸€éšè—å±‚è¯¯å·®
        error1 = np.dot(delta2, self.w2.T)
        delta1 = error1 * self.relu_derivative(self.z1)
        
        # å½’ä¸€åŒ–çŠ¶æ€
        normalized_state = np.array([
            state[0] / 10.0, state[1] / 10.0, state[2] / 10.0,
            state[3] / math.pi, state[4] / 5.0
        ])
        
        # æ›´æ–°æƒé‡
        self.w3 -= self.learning_rate * np.outer(self.a2, delta3)
        self.b3 -= self.learning_rate * delta3
        
        self.w2 -= self.learning_rate * np.outer(self.a1, delta2)
        self.b2 -= self.learning_rate * delta2
        
        self.w1 -= self.learning_rate * np.outer(normalized_state, delta1)
        self.b1 -= self.learning_rate * delta1

class ParameterOptimizationTurtle(Node):
    def __init__(self, turtle_name):
        super().__init__(f'{turtle_name}_param_opt')
        
        # ROS2è®¾ç½®
        self.pose_receiver = self.create_subscription(
            Pose, f'/{turtle_name}/pose', self.pose_callback, 10)
        self.cmd_sender = self.create_publisher(
            Twist, f'/{turtle_name}/cmd_vel', 10)
        
        # æ§åˆ¶å‚æ•°
        self.target_route = [(2, 2), (9, 2), (9, 9), (2, 9)]
        self.current_index = 0
        self.current_pose = None
        
        # å‚æ•°åŒ–ç­–ç•¥å’Œç¥ç»ç½‘ç»œ
        self.policy = ParameterizedPolicy()
        self.param_network = PolicyParameterNetwork()
        
        # æ€§èƒ½è¯„ä¼°
        self.last_distance = None
        self.performance_history = []
        self.targets_reached = 0
        
        # è®­ç»ƒæ•°æ®æ”¶é›†
        self.training_states = []
        self.training_adjustments = []
        
        # æ§åˆ¶å¾ªç¯
        self.create_timer(0.1, self.control_loop)
        
        self.get_logger().info('ğŸ›ï¸ å‚æ•°åŒ–ç­–ç•¥ä¼˜åŒ–æµ·é¾Ÿå¯åŠ¨ï¼')
        self.get_logger().info(f'ğŸ“Š åˆå§‹å‚æ•°: {self.policy.params}')

    def pose_callback(self, msg):
        self.current_pose = msg
    
    def evaluate_performance(self, old_distance, new_distance, reached_target):
        """è¯„ä¼°å½“å‰æ€§èƒ½"""
        performance = 0
        
        # è·ç¦»æ”¹å–„å¥–åŠ±
        if old_distance is not None:
            performance += (old_distance - new_distance) * 10
        
        # åˆ°è¾¾ç›®æ ‡å¤§å¥–åŠ±
        if reached_target:
            performance += 100
        
        # è·ç¦»æƒ©ç½š
        performance -= new_distance * 0.5
        
        return performance
    
    def generate_parameter_adjustments(self, performance):
        """åŸºäºæ€§èƒ½ç”Ÿæˆå‚æ•°è°ƒæ•´å»ºè®®"""
        # ç®€å•çš„æ€§èƒ½å¯¼å‘è°ƒæ•´ç­–ç•¥
        adjustments = {}
        
        if performance > 50:  # è¡¨ç°å¾ˆå¥½
            # å¯ä»¥å°è¯•æé«˜é€Ÿåº¦
            adjustments['max_speed'] = 0.1
            adjustments['speed_distance_factor'] = 0.05
        elif performance < -10:  # è¡¨ç°ä¸å¥½
            # æ›´ä¿å®ˆçš„ç­–ç•¥
            adjustments['angle_threshold'] = -0.01  # æ›´æ—©è½¬å‘
            adjustments['turn_speed'] = -0.1  # è½¬å‘æ›´æ…¢
            adjustments['slow_factor'] = -0.02  # æ›´æ—©å‡é€Ÿ
        
        # è½¬æ¢ä¸ºå‘é‡å½¢å¼
        param_names = list(self.policy.params.keys())
        adjustment_vector = np.array([
            adjustments.get(name, 0.0) for name in param_names
        ])
        
        return adjustment_vector
    
    def control_loop(self):
        """ä¸»æ§åˆ¶å¾ªç¯"""
        if not self.current_pose:
            return
        
        target = self.target_route[self.current_index]
        
        # è®¡ç®—çŠ¶æ€
        dx = target[0] - self.current_pose.x
        dy = target[1] - self.current_pose.y
        distance = math.sqrt(dx*dx + dy*dy)
        
        target_angle = math.atan2(dy, dx)
        angle_diff = target_angle - self.current_pose.theta
        while angle_diff > math.pi:
            angle_diff -= 2 * math.pi
        while angle_diff < -math.pi:
            angle_diff += 2 * math.pi
        
        current_speed = math.sqrt(self.current_pose.linear_velocity**2 + 
                                 self.current_pose.angular_velocity**2)
        
        current_state = [dx, dy, distance, angle_diff, current_speed]
        
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
        reached_target = distance < 0.3
        if reached_target:
            self.targets_reached += 1
            self.get_logger().info(f'ğŸ¯ åˆ°è¾¾ç›®æ ‡ {target} (ç¬¬{self.targets_reached}ä¸ª)')
            self.current_index = (self.current_index + 1) % len(self.target_route)
            return
        
        # è¯„ä¼°æ€§èƒ½å¹¶æ”¶é›†è®­ç»ƒæ•°æ®
        if self.last_distance is not None:
            performance = self.evaluate_performance(self.last_distance, distance, reached_target)
            self.performance_history.append(performance)
            
            # ç”Ÿæˆå‚æ•°è°ƒæ•´å»ºè®®
            target_adjustments = self.generate_parameter_adjustments(performance)
            
            # æ”¶é›†è®­ç»ƒæ•°æ®
            self.training_states.append(current_state.copy())
            self.training_adjustments.append(target_adjustments.copy())
            
            # é™åˆ¶è®­ç»ƒæ•°æ®å¤§å°
            if len(self.training_states) > 1000:
                self.training_states.pop(0)
                self.training_adjustments.pop(0)
        
        # ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹å‚æ•°è°ƒæ•´
        if len(self.training_states) > 50:
            param_adjustments = self.param_network.forward(current_state)
            # å°†è°ƒæ•´åº”ç”¨åˆ°ç­–ç•¥å‚æ•°
            param_names = list(self.policy.params.keys())
            adjustment_dict = {name: param_adjustments[i] for i, name in enumerate(param_names)}
            self.policy.update_params(adjustment_dict)
        
        # ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆæ§åˆ¶å‘½ä»¤
        v, w = self.policy.generate_action(dx, dy, distance, angle_diff, current_speed)
        
        # å‘é€æ§åˆ¶å‘½ä»¤
        cmd = Twist()
        cmd.linear.x = float(v)
        cmd.angular.z = float(w)
        self.cmd_sender.publish(cmd)
        
        self.last_distance = distance
        
        # å®šæœŸè®­ç»ƒç½‘ç»œ
        if len(self.training_states) > 100 and len(self.training_states) % 50 == 0:
            self.train_parameter_network()
        
        # å®šæœŸæ˜¾ç¤ºçŠ¶æ€
        if len(self.performance_history) > 0 and len(self.performance_history) % 100 == 0:
            avg_performance = np.mean(self.performance_history[-50:])
            self.get_logger().info(f'ğŸ“ˆ å¹³å‡æ€§èƒ½: {avg_performance:.2f}, å‚æ•°æ ·æœ¬: {self.policy.params}')
    
    def train_parameter_network(self):
        """è®­ç»ƒå‚æ•°è°ƒæ•´ç½‘ç»œ"""
        if len(self.training_states) < 32:
            return
        
        # éšæœºé‡‡æ ·è®­ç»ƒæ‰¹æ¬¡
        indices = np.random.choice(len(self.training_states), 32, replace=False)
        
        total_loss = 0
        for idx in indices:
            state = self.training_states[idx]
            target_adj = self.training_adjustments[idx]
            
            # å‰å‘ä¼ æ’­
            predicted_adj = self.param_network.forward(state)
            
            # è®¡ç®—æŸå¤±
            loss = np.mean((predicted_adj - target_adj)**2)
            total_loss += loss
            
            # åå‘ä¼ æ’­
            self.param_network.backward(state, target_adj)
        
        avg_loss = total_loss / 32
        self.get_logger().info(f'ğŸ§  å‚æ•°ç½‘ç»œè®­ç»ƒæŸå¤±: {avg_loss:.6f}')

def main():
    rclpy.init()
    
    try:
        turtle = ParameterOptimizationTurtle('turtle1')
        print("ğŸ›ï¸ å‚æ•°åŒ–ç­–ç•¥ä¼˜åŒ–æµ·é¾Ÿå¯åŠ¨ï¼")
        print("ğŸ’¡ ç¥ç»ç½‘ç»œå°†å­¦ä¹ å¦‚ä½•ä¼˜åŒ–æ§åˆ¶ç­–ç•¥çš„å‚æ•°")
        print("ğŸ“Š è§‚å¯Ÿç­–ç•¥å‚æ•°å¦‚ä½•æ ¹æ®æ€§èƒ½è‡ªé€‚åº”è°ƒæ•´")
        
        rclpy.spin(turtle)
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ åœæ­¢ä¼˜åŒ–")
    
    finally:
        if 'turtle' in locals():
            turtle.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()